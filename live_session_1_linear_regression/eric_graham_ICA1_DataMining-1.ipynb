{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35c190f8",
   "metadata": {
    "id": "XghjcvFIZ6gk"
   },
   "source": [
    "**NAME**: Eric Graham"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09586246",
   "metadata": {
    "id": "KuSObsjMZ6gn"
   },
   "source": [
    "# In Class Assignment 1\n",
    "In the following assignment you will be asked to fill in python code and derivations for a number of different problems.\n",
    "**Please fill in the code and answer all the questions asked.** Please read all instructions carefully and turn in the rendered notebook after class.\n",
    "\n",
    "### Loading the Data\n",
    "Diabetes Dataset Details: https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset\n",
    "\n",
    "Please run the following code to read in the diabetes dataset using the sklearn data loading module.\n",
    "\n",
    "This will load the data into the variable `ds`. Note that `ds` is a dictionary object with fields like `ds.data`, which is a matrix of the continuous features in the dataset. The object is not a pandas dataframe. It is a numpy matrix. Each row is a set of observed instances, each column is a different feature. It also has a field called `ds.target` that is a continuous value we are trying to predict. Each entry in `ds.target` is a label for each row of the `ds.data` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "855ddae1",
   "metadata": {
    "id": "EwhUuBKYZ6go"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "from pprint import pprint #for \"pretty\" printing\n",
    "\n",
    "from sklearn.datasets import load_diabetes # sklearn data\n",
    "from sklearn.linear_model import LinearRegression #sklearn linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83bb4633",
   "metadata": {
    "id": "eZzDTyxGZ6gp",
    "outputId": "a47ddd7b-5ce4-4686-bd61-31f6ff68d44d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type is:  <class 'numpy.ndarray'>\n",
      "dataset shape: (442, 10) format is: ('rows', 'columns')\n",
      "range of target: 25.0 346.0\n"
     ]
    }
   ],
   "source": [
    "ds = load_diabetes()\n",
    "\n",
    "# this holds the data which consists of continuous features\n",
    "# because ds.data is a matrix, there are some special properties we can access, like shape (see below)\n",
    "print('type is: ', type(ds.data))\n",
    "print('dataset shape:', ds.data.shape, 'format is:', ('rows','columns')) # there are 442 instances with 10 features each\n",
    "print('range of target:', np.min(ds.target),np.max(ds.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93b97b29",
   "metadata": {
    "id": "19ab7T1kZ6gq",
    "outputId": "5245fcfb-a366-45e0-e621-682333f1e06a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
      "         0.01990749, -0.01764613],\n",
      "       [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
      "        -0.06833155, -0.09220405],\n",
      "       [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
      "         0.00286131, -0.02593034],\n",
      "       ...,\n",
      "       [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
      "        -0.04688253,  0.01549073],\n",
      "       [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
      "         0.04452873, -0.02593034],\n",
      "       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
      "        -0.00422151,  0.00306441]], shape=(442, 10))\n",
      "array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
      "        69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
      "        68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
      "        87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
      "       259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
      "       128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
      "       150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
      "       200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
      "        42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
      "        83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
      "       104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
      "       173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
      "       107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
      "        60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
      "       197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
      "        59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
      "       237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
      "       143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
      "       142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
      "        77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
      "        78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
      "       154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
      "        71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
      "       150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
      "       145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
      "        94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
      "        60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
      "        31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
      "       114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
      "       191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
      "       244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
      "       263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
      "        77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
      "        58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
      "       140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
      "       219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
      "        43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
      "       140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
      "        84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
      "        94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
      "       220.,  57.])\n"
     ]
    }
   ],
   "source": [
    "# we can set the fields inside of ds and set them to new variables in python\n",
    "pprint(ds.data) # prints out elements of the matrix\n",
    "pprint(ds.target) # prints the vector (all 442 items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4158fddf",
   "metadata": {
    "id": "B9o68c2VZ6gr"
   },
   "source": [
    "### Using Linear Regression\n",
    "In the videos, we derived the formula for calculating the optimal values of the regression weights as\n",
    "\n",
    "$$ w = (X^TX)^{-1}X^Ty, $$\n",
    "\n",
    "where $X$ is the matrix of values with a bias column of ones appended onto it. For the diabetes dataset one could construct this $X$ matrix by stacking a column of ones onto the `ds.data` matrix.\n",
    "\n",
    "$$ X=\\begin{bmatrix}\n",
    "         & \\vdots &        &  1 \\\\\n",
    "        \\dotsb & \\text{ds.data} & \\dotsb &  \\vdots\\\\\n",
    "         & \\vdots &         &  1\\\\\n",
    "     \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "# Question 1:\n",
    "For the diabetes dataset, how many elements will the vector $w$ contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59eee101-4c5b-4727-9876-ea8cf9b08fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb14767-2c41-49de-af3c-ec6880d8a108",
   "metadata": {
    "id": "v09hxy18Z6gr"
   },
   "source": [
    "## Answer:\n",
    "\n",
    "W will contain a number of elements equal to the number of features plus a bias term. Since our dataset has 10 features, W will contain 11 elements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92f1e56",
   "metadata": {
    "id": "RDPUAUQjZ6gs"
   },
   "source": [
    "# Question 2:\n",
    "\n",
    "In the following empty cell, use the above equation and numpy matrix operations to find the values of the vector $w$. You will need to be sure $X$ and $y$ are created like the instructor talked about in the video. Don't forget to include any modifications to $X$ to account for the bias term in $w$. You might be interested in the following functions:\n",
    "\n",
    "- `np.hstack((mat1,mat2))` stack two matrices horizontally, to create a new matrix\n",
    "- `np.ones((rows,cols))` create a matrix full of ones\n",
    "- `my_mat.T` takes transpose of numpy matrix named `my_mat`\n",
    "- `np.dot(mat1,mat2)` is matrix multiplication for two matrices\n",
    "- `np.linalg.inv(mat)` gets the inverse of the variable `mat`\n",
    "\n",
    "## Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dc101e1",
   "metadata": {
    "id": "Xms_OSfMZ6gs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -10.0098663  -239.81564367  519.84592005  324.3846455  -792.17563855\n",
      "  476.73902101  101.04326794  177.06323767  751.27369956   67.62669218\n",
      "  152.13348416]\n"
     ]
    }
   ],
   "source": [
    "# Write you code here, print the values of the regression weights using the 'print()' function in python\n",
    "X = np.hstack((ds.data, np.ones((ds.data.shape[0], 1))))\n",
    "y = ds.target\n",
    "w = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b349a5",
   "metadata": {
    "id": "r75MZNIMZ6gs"
   },
   "source": [
    "# Question 3:\n",
    "\n",
    "Scikit-learn also has a linear regression fitting implementation. Look at the scikit learn API and learn to use the linear regression method. The API is here:\n",
    "\n",
    "- API Reference: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "Use the sklearn `LinearRegression` module to fit a LinearRegression model  to check your results from the previous question. Did you get the same parameters?\n",
    "\n",
    "## Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "855939b4",
   "metadata": {
    "id": "1oq4HPBmZ6gt",
    "outputId": "79bac95a-a599-494d-baa5-54ba2003946a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model coefficients are: [ -10.0098663  -239.81564367  519.84592005  324.3846455  -792.17563855\n",
      "  476.73902101  101.04326794  177.06323767  751.27369956   67.62669218\n",
      "    0.        ]\n",
      "model intercept is 152.13348416289597\n",
      "Answer to question is Yes they are the same!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# write your code here, print the values of model by accessing\n",
    "#    its properties that you looked up from the API\n",
    "\n",
    "fit = LinearRegression().fit(X, y)\n",
    "\n",
    "\n",
    "print('model coefficients are:', fit.coef_)\n",
    "print('model intercept is', fit.intercept_)\n",
    "print('Answer to question is', 'Yes they are the same!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5b5b7c",
   "metadata": {
    "id": "8SNcCsn4Z6gt"
   },
   "source": [
    "# Question 4:\n",
    "\n",
    "Recall that to predict the output from our model, $\\hat{y}$, from $w$ and $X$ we need to use the following formula\n",
    "\\begin{align}\n",
    "    \\hat{y}=w^TX^T,\n",
    "\\end{align}\n",
    "\n",
    "where $X$ is a matrix with example instances in *each row* of the matrix.\n",
    "\n",
    "\n",
    "- **Part A:** Compute $\\hat{y}$ in two ways: 1) By matrix multiplication using numpy and the above equation (denote this with $\\hat{y}_{numpy}$) and 2)  by using the sklearn regression model from Question 3 (denote this $\\hat{y}_{sklearn}$).\n",
    "        \n",
    "    Note: you may need to make the regression weights a column vector using the following code: `w = w.reshape((len(w),1))` This assumes your weights vector is assigned to the variable named `w`.\n",
    "- **Part B:** Calculate the mean squared error between your prediction from numpy and the target: $\\sum_i(y-\\hat{y}_{numpy})^2$.\n",
    "- **Part C:** Calculate the mean squared error between your sklearn prediction and the target: $\\sum_i(y-\\hat{y}_{sklearn})^2$.\n",
    "\n",
    "## Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd7a4186",
   "metadata": {
    "id": "Dk8GzkIWZ6gt",
    "outputId": "b0c1ca26-2daa-42bc-ba58-0b057f4adbb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Sklearn is: [206.11667725  68.07103297 176.88279035 166.91445843 128.46225834\n",
      " 106.35191443  73.89134662 118.85423042 158.80889721 213.58462442\n",
      "  97.07481511  95.10108423 115.06915952 164.67656842 103.07814257\n",
      " 177.17487964 211.7570922  182.84134823 148.00326937 124.01754066\n",
      " 120.33362197  85.80068961 113.1134589  252.45225837 165.48779206\n",
      " 147.71997564  97.12871541 179.09358468 129.05345958 184.7811403\n",
      " 158.71516713  69.47575778 261.50385365 112.82234716  78.37318279\n",
      "  87.66360785 207.92114668 157.87641942 240.84708073 136.93257456\n",
      " 153.48044608  74.15426666 145.62742227  77.82978811 221.07832768\n",
      " 125.21957584 142.6029986  109.49562511  73.14181818 189.87117754\n",
      " 157.9350104  169.55699526 134.1851441  157.72539008 139.11104979\n",
      "  72.73116856 207.82676612  80.11171342 104.08335958 134.57871054\n",
      " 114.23552012 180.67628279  61.12935368  98.72404613 113.79577026\n",
      " 189.95771575 148.98351571 124.34152283 114.8395504  121.99957578\n",
      "  73.91017087 236.71054289 142.31126791 124.51672384 150.84073896\n",
      " 127.75230658 191.16896496  77.05671154 166.82164929  91.00591229\n",
      " 174.75156797 122.83451589  63.27231315 151.99867317  53.72959077\n",
      " 166.0050229   42.6491333  153.04229493  80.54701716 106.90148495\n",
      "  79.93968011 187.1672654  192.5989033   61.07398313 107.4076912\n",
      " 125.04307496 207.72402726 214.21248827 123.47464895 139.16439034\n",
      " 168.21372017 106.92902558 150.64748328 157.92364009 152.75958287\n",
      " 116.22381927  73.03167734 155.67052006 230.1417777  143.49797317\n",
      "  38.09587272 121.8593267  152.79404663 207.99702587 291.23106133\n",
      " 189.17571129 214.02877593 235.18106509 165.38480498 151.2469168\n",
      " 156.57659557 200.44066818 219.35193167 174.78830391 169.23118221\n",
      " 187.87537099  57.49340026 108.54836058  92.68731024 210.87347343\n",
      " 245.47097701  69.84285129 113.03485904  68.42650654 141.69639374\n",
      " 239.46240737  58.37858726 235.47123197 254.92309543 253.30708899\n",
      " 155.51063293 230.55961445 170.44330954 117.9953395  178.55406527\n",
      " 240.07119308 190.33892524 228.66470581 114.24456339 178.36552308\n",
      " 209.091817   144.85615197 200.65926745 121.34295733 150.50993019\n",
      " 199.01879825 146.27926469 124.02163345  85.25913019 235.16173729\n",
      "  82.1730808  231.29474031 144.36940116 197.04628448 146.99841953\n",
      "  77.18813284  59.37368356 262.68557988 225.12900796 220.20301952\n",
      "  46.59651844  88.10194612 221.77450036  97.25199783 164.48838425\n",
      " 119.90096817 157.80220788 223.08012207  99.59081773 165.84386951\n",
      " 179.47680741  89.83353846 171.82590335 158.36419935 201.48185539\n",
      " 186.39194958 197.47424761  66.57371647 154.59985312 116.18319159\n",
      " 195.91755793 128.04834496  91.20395862 140.57223765 155.22669143\n",
      " 169.70326581  98.7573858  190.14568824 142.51704894 177.27157771\n",
      "  95.30812216  69.06191507 164.16391317 198.0659024  178.25996632\n",
      " 228.58539684 160.67104137 212.28734795 222.4833913  172.85421282\n",
      " 125.27946793 174.72103207 152.38094643  98.58135665  99.73771331\n",
      " 262.29507095 223.74033222 221.33976142 133.61470602 145.42828204\n",
      "  53.04569008 141.82052358 153.68617582 125.22290891  77.25168449\n",
      " 230.26180811  78.9090807  105.2051755  117.99622779  99.06233889\n",
      " 166.55796947 159.34137227 158.27448255 143.05684078 231.55890118\n",
      " 176.64724258 187.23580712  65.39099908 190.66218796 179.75181691\n",
      " 234.9080532  119.15669025  85.63551834 100.8597527  140.41937377\n",
      " 101.83524022 120.66560385  83.0664276  234.58488012 245.15862773\n",
      " 263.26954282 274.87127261 180.67257769 203.05642297 254.21625849\n",
      " 118.44300922 268.45369506 104.83843473 115.86820464 140.45857194\n",
      "  58.46948192 129.83145265 263.78607272  45.00934573 123.28890007\n",
      " 131.0856888   34.89181681 138.35467112 244.30103923  89.95923929\n",
      " 192.07096194 164.33017386 147.74779723 191.89092557 176.44360299\n",
      " 158.3490221  189.19166962 116.58117777 111.449754   117.45232726\n",
      " 165.79598354  97.80405886 139.54451791  84.17319946 159.93677518\n",
      " 202.39971737  80.48131518 146.64558568  79.05314048 191.33777472\n",
      " 220.67516721 203.75017281  92.86459928 179.15576252  81.79874055\n",
      " 152.8290929   76.80052219  97.79590831 106.8371012  123.83461591\n",
      " 218.13908293 126.01937664 206.7587966  230.5767944  122.05921633\n",
      " 135.67824405 126.37042532 148.49374458  88.07147107 138.95823614\n",
      " 203.8691938  172.55288732 122.95701477 213.92310163 174.89158814\n",
      " 110.07294222 198.36584973 173.25229067 162.64748776 193.31578983\n",
      " 191.53493643 284.13932209 279.31133207 216.00823829 210.08668656\n",
      " 216.21612991 157.01450004 224.06431372 189.06103154 103.56515315\n",
      " 178.70270016 111.81862434 291.00196609 182.64651752  79.33315426\n",
      "  86.33029851 249.1510082  174.51537682 122.10291074 146.2718871\n",
      " 170.65483847 183.497196   163.36806262 157.03297709 144.42614949\n",
      " 125.30053093 177.50251197 104.57681546 132.17560518  95.06210623\n",
      " 249.89755705  86.23824126  61.99847009 156.81295053 192.32218372\n",
      " 133.85525804  93.67249793 202.49572354  52.54148927 174.82799914\n",
      " 196.91468873 118.06336979 235.29941812 165.09438096 160.41761959\n",
      " 162.37786753 254.05587268 257.23492156 197.5039462  184.06877122\n",
      "  58.62131994 194.39216636 110.775815   142.20991224 128.82520996\n",
      " 180.13082199 211.26488624 169.59494046 164.33851796 136.23374077\n",
      " 174.51001028  74.67587343 246.29432383 114.14494406 111.54552901\n",
      " 140.0224376  109.99895704  91.37283987 163.01540596  75.16804478\n",
      " 254.06119047  53.47338214  98.48397565 100.66315554 258.58683032\n",
      " 170.67256752  61.91771186 182.31148421 171.26948629 189.19505093\n",
      " 187.18494664  87.12170524 148.37964317 251.35815403 199.69656904\n",
      " 283.63576862  50.85911237 172.14766276 204.05976093 174.16540137\n",
      " 157.93182911 150.50028158 232.97445368 121.5814873  164.54245461\n",
      " 172.67625919 226.7768891  149.46832104  99.13924946  80.43418456\n",
      " 140.16148637 191.90710484 199.28001608 153.63277325 171.80344337\n",
      " 112.11054883 162.60002916 129.84290324 258.03100468 100.70810916\n",
      " 115.87608197 122.53559675 218.1797988   60.94350929 131.09296884\n",
      " 119.48376601  52.60911672 193.01756549 101.05581371 121.22668124\n",
      " 211.85894518  53.44727472]\n",
      "MSE Numpy is: [[206.11667725  68.07103297 176.88279035 166.91445843 128.46225834\n",
      "  106.35191443  73.89134662 118.85423042 158.80889721 213.58462442\n",
      "   97.07481511  95.10108423 115.06915952 164.67656842 103.07814257\n",
      "  177.17487964 211.7570922  182.84134823 148.00326937 124.01754066\n",
      "  120.33362197  85.80068961 113.1134589  252.45225837 165.48779206\n",
      "  147.71997564  97.12871541 179.09358468 129.05345958 184.7811403\n",
      "  158.71516713  69.47575778 261.50385365 112.82234716  78.37318279\n",
      "   87.66360785 207.92114668 157.87641942 240.84708073 136.93257456\n",
      "  153.48044608  74.15426666 145.62742227  77.82978811 221.07832768\n",
      "  125.21957584 142.6029986  109.49562511  73.14181818 189.87117754\n",
      "  157.9350104  169.55699526 134.1851441  157.72539008 139.11104979\n",
      "   72.73116856 207.82676612  80.11171342 104.08335958 134.57871054\n",
      "  114.23552012 180.67628279  61.12935368  98.72404613 113.79577026\n",
      "  189.95771575 148.98351571 124.34152283 114.8395504  121.99957578\n",
      "   73.91017087 236.71054289 142.31126791 124.51672384 150.84073896\n",
      "  127.75230658 191.16896496  77.05671154 166.82164929  91.00591229\n",
      "  174.75156797 122.83451589  63.27231315 151.99867317  53.72959077\n",
      "  166.0050229   42.6491333  153.04229493  80.54701716 106.90148495\n",
      "   79.93968011 187.1672654  192.5989033   61.07398313 107.4076912\n",
      "  125.04307496 207.72402726 214.21248827 123.47464895 139.16439034\n",
      "  168.21372017 106.92902558 150.64748328 157.92364009 152.75958287\n",
      "  116.22381927  73.03167734 155.67052006 230.1417777  143.49797317\n",
      "   38.09587272 121.8593267  152.79404663 207.99702587 291.23106133\n",
      "  189.17571129 214.02877593 235.18106509 165.38480498 151.2469168\n",
      "  156.57659557 200.44066818 219.35193167 174.78830391 169.23118221\n",
      "  187.87537099  57.49340026 108.54836058  92.68731024 210.87347343\n",
      "  245.47097701  69.84285129 113.03485904  68.42650654 141.69639374\n",
      "  239.46240737  58.37858726 235.47123197 254.92309543 253.30708899\n",
      "  155.51063293 230.55961445 170.44330954 117.9953395  178.55406527\n",
      "  240.07119308 190.33892524 228.66470581 114.24456339 178.36552308\n",
      "  209.091817   144.85615197 200.65926745 121.34295733 150.50993019\n",
      "  199.01879825 146.27926469 124.02163345  85.25913019 235.16173729\n",
      "   82.1730808  231.29474031 144.36940116 197.04628448 146.99841953\n",
      "   77.18813284  59.37368356 262.68557988 225.12900796 220.20301952\n",
      "   46.59651844  88.10194612 221.77450036  97.25199783 164.48838425\n",
      "  119.90096817 157.80220788 223.08012207  99.59081773 165.84386951\n",
      "  179.47680741  89.83353846 171.82590335 158.36419935 201.48185539\n",
      "  186.39194958 197.47424761  66.57371647 154.59985312 116.18319159\n",
      "  195.91755793 128.04834496  91.20395862 140.57223765 155.22669143\n",
      "  169.70326581  98.7573858  190.14568824 142.51704894 177.27157771\n",
      "   95.30812216  69.06191507 164.16391317 198.0659024  178.25996632\n",
      "  228.58539684 160.67104137 212.28734795 222.4833913  172.85421282\n",
      "  125.27946793 174.72103207 152.38094643  98.58135665  99.73771331\n",
      "  262.29507095 223.74033222 221.33976142 133.61470602 145.42828204\n",
      "   53.04569008 141.82052358 153.68617582 125.22290891  77.25168449\n",
      "  230.26180811  78.9090807  105.2051755  117.99622779  99.06233889\n",
      "  166.55796947 159.34137227 158.27448255 143.05684078 231.55890118\n",
      "  176.64724258 187.23580712  65.39099908 190.66218796 179.75181691\n",
      "  234.9080532  119.15669025  85.63551834 100.8597527  140.41937377\n",
      "  101.83524022 120.66560385  83.0664276  234.58488012 245.15862773\n",
      "  263.26954282 274.87127261 180.67257769 203.05642297 254.21625849\n",
      "  118.44300922 268.45369506 104.83843473 115.86820464 140.45857194\n",
      "   58.46948192 129.83145265 263.78607272  45.00934573 123.28890007\n",
      "  131.0856888   34.89181681 138.35467112 244.30103923  89.95923929\n",
      "  192.07096194 164.33017386 147.74779723 191.89092557 176.44360299\n",
      "  158.3490221  189.19166962 116.58117777 111.449754   117.45232726\n",
      "  165.79598354  97.80405886 139.54451791  84.17319946 159.93677518\n",
      "  202.39971737  80.48131518 146.64558568  79.05314048 191.33777472\n",
      "  220.67516721 203.75017281  92.86459928 179.15576252  81.79874055\n",
      "  152.8290929   76.80052219  97.79590831 106.8371012  123.83461591\n",
      "  218.13908293 126.01937664 206.7587966  230.5767944  122.05921633\n",
      "  135.67824405 126.37042532 148.49374458  88.07147107 138.95823614\n",
      "  203.8691938  172.55288732 122.95701477 213.92310163 174.89158814\n",
      "  110.07294222 198.36584973 173.25229067 162.64748776 193.31578983\n",
      "  191.53493643 284.13932209 279.31133207 216.00823829 210.08668656\n",
      "  216.21612991 157.01450004 224.06431372 189.06103154 103.56515315\n",
      "  178.70270016 111.81862434 291.00196609 182.64651752  79.33315426\n",
      "   86.33029851 249.1510082  174.51537682 122.10291074 146.2718871\n",
      "  170.65483847 183.497196   163.36806262 157.03297709 144.42614949\n",
      "  125.30053093 177.50251197 104.57681546 132.17560518  95.06210623\n",
      "  249.89755705  86.23824126  61.99847009 156.81295053 192.32218372\n",
      "  133.85525804  93.67249793 202.49572354  52.54148927 174.82799914\n",
      "  196.91468873 118.06336979 235.29941812 165.09438096 160.41761959\n",
      "  162.37786753 254.05587268 257.23492156 197.5039462  184.06877122\n",
      "   58.62131994 194.39216636 110.775815   142.20991224 128.82520996\n",
      "  180.13082199 211.26488624 169.59494046 164.33851796 136.23374077\n",
      "  174.51001028  74.67587343 246.29432383 114.14494406 111.54552901\n",
      "  140.0224376  109.99895704  91.37283987 163.01540596  75.16804478\n",
      "  254.06119047  53.47338214  98.48397565 100.66315554 258.58683032\n",
      "  170.67256752  61.91771186 182.31148421 171.26948629 189.19505093\n",
      "  187.18494664  87.12170524 148.37964317 251.35815403 199.69656904\n",
      "  283.63576862  50.85911237 172.14766276 204.05976093 174.16540137\n",
      "  157.93182911 150.50028158 232.97445368 121.5814873  164.54245461\n",
      "  172.67625919 226.7768891  149.46832104  99.13924946  80.43418456\n",
      "  140.16148637 191.90710484 199.28001608 153.63277325 171.80344337\n",
      "  112.11054883 162.60002916 129.84290324 258.03100468 100.70810916\n",
      "  115.87608197 122.53559675 218.1797988   60.94350929 131.09296884\n",
      "  119.48376601  52.60911672 193.01756549 101.05581371 121.22668124\n",
      "  211.85894518  53.44727472]]\n",
      "1263985.7856333437\n",
      "1263985.7856333435\n"
     ]
    }
   ],
   "source": [
    "# Use this block to answer the questions\n",
    "# Part A\n",
    "w = w.reshape((len(w),1)) # make w a column vector\n",
    "y_numpy = np.dot(w.T, X.T)\n",
    "y_sklearn = fit.predict(X)\n",
    "\n",
    "# print(w.shape)\n",
    "# print(X.shape)\n",
    "\n",
    "print('MSE Sklearn is:', y_sklearn)\n",
    "print('MSE Numpy is:', y_numpy)\n",
    "\n",
    "# Part B\n",
    "diff_numpy = ds.target - y_numpy\n",
    "squared_diff_numpy = diff_numpy ** 2\n",
    "mse_numpy = np.sum(squared_diff_numpy)\n",
    "print(mse_numpy)\n",
    "\n",
    "# Part C\n",
    "diff_sklearn = ds.target - y_sklearn\n",
    "squared_diff_sklearn = diff_sklearn ** 2\n",
    "mse_sklearn = np.sum(squared_diff_sklearn)\n",
    "print(mse_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa0f66b",
   "metadata": {
    "id": "Q4jG6xJFZ6gu"
   },
   "source": [
    "# Question 5: Using Linear Classification\n",
    "Now lets use the code you created to make a classifier with linear boundaries. Run the following code in order to load the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd741128",
   "metadata": {
    "id": "dtBdum82Z6gu",
    "outputId": "bfe78ad4-b8d8-4fb8-aeb5-bfbdeb096592"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (150, 4)\n",
      "original number of classes: 3\n",
      "new number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# this will overwrite the diabetes dataset\n",
    "iris_ds = load_iris()\n",
    "print('features shape:', iris_ds.data.shape) # there are 150 instances and 4 features per instance\n",
    "print('original number of classes:', len(np.unique(iris_ds.target)))\n",
    "\n",
    "# now let's make this a binary classification task\n",
    "iris_ds.target = iris_ds.target>1\n",
    "print ('new number of classes:', len(np.unique(iris_ds.target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67f709b",
   "metadata": {
    "id": "5Jfzrx7JZ6gu"
   },
   "source": [
    "**Question 5:** Now use linear regression to come up with a set of weights, `w`, that predict the class value. This is exactly like you did before for the *diabetes* dataset. However, instead of regressing to continuous values, you are just regressing to the integer value of the class (0 or 1), like we talked about in the video. Remember to account for the bias term when constructing the feature matrix, `X`. Print the weights of the linear classifier.\n",
    "\n",
    "## Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9fbda8f",
   "metadata": {
    "id": "5kKUJvZGZ6gu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04587608  0.20276839  0.00398791  0.55177932 -0.69528186]\n"
     ]
    }
   ],
   "source": [
    "# write your code here and print the values of the weights\n",
    "X_iris = np.hstack((iris_ds.data, np.ones((iris_ds.data.shape[0], 1))))\n",
    "y_iris = iris_ds.target.astype(int) \n",
    "w_iris = np.dot(np.dot(np.linalg.inv(np.dot(X_iris.T, X_iris)), X_iris.T), y_iris)\n",
    "\n",
    "print(w_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e568ec7a",
   "metadata": {
    "id": "raelEovMZ6gv"
   },
   "source": [
    "# Question 6:\n",
    "\n",
    "Finally, use a hard decision function on the output of the linear regression to make this a binary classifier. This is just like we talked about in the video, where the output of the linear regression passes through a function $g$ in the form:\n",
    "\n",
    "$$\\hat{y}=g(w^TX^T), \\text{where} $$\n",
    " - $g(w^TX^T)$ for $w^TX^T < \\alpha$ maps the predicted class to `0`,\n",
    " - $g(w^TX^T)$ for $w^TX^T \\geq \\alpha$ maps the predicted class to `1`.\n",
    "\n",
    "$\\alpha$ is a threshold for deciding the class.\n",
    "\n",
    "What value for $\\alpha$ makes the most sense? What is the accuracy of the classifier given the $\\alpha$ you chose?\n",
    "\n",
    "Note: You can calculate the accuracy with as follows: `accuracy = float(sum(yhat==y)) / len(y)` where `y` and `yhat` denote the true targets and the predicted targets, respectively.\n",
    "\n",
    "## Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fd9046bc",
   "metadata": {
    "id": "H2JfMwPhZ6gv",
    "outputId": "f032d674-9071-489b-b32d-5cc0e8da8bcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Alpha 0.5: 0.9266666666666666\n",
      "Accuracy with Alpha 0.0: 0.6066666666666667\n",
      "Accuracy with Alpha 0.25: 0.7466666666666667\n",
      "Accuracy with Alpha 0.75: 0.8266666666666667\n",
      "Accuracy with Alpha 0.95: 0.7266666666666667\n",
      "The best accuracy comes from Alpha 0.5: 0.9266666666666666\n"
     ]
    }
   ],
   "source": [
    "# use this box to predict the classification output\n",
    "\n",
    "predictions_iris = np.dot(w_iris.T, X_iris.T)\n",
    "\n",
    "# let's try 0.5 first\n",
    "\n",
    "alpha = 0.5\n",
    "predictions_iris_a05 = (predictions_iris >= alpha).astype(int)\n",
    "\n",
    "accuracy_a05 = float(sum(predictions_iris_a05 == y_iris)) / len(y_iris)\n",
    "print('Accuracy with Alpha 0.5:', accuracy_a05)\n",
    "\n",
    "# alpha = 0.0\n",
    "alpha = 0.0\n",
    "predictions_iris_a0 = (predictions_iris >= alpha).astype(int)\n",
    "\n",
    "accuracy_a00 = float(sum(predictions_iris_a0 == y_iris)) / len(y_iris)\n",
    "print('Accuracy with Alpha 0.0:', accuracy_a00)\n",
    "\n",
    "# alpha = 0.25\n",
    "alpha = 0.25\n",
    "predictions_iris_a025 = (predictions_iris >= alpha).astype(int)\n",
    "\n",
    "accuracy_a025 = float(sum(predictions_iris_a025 == y_iris)) / len(y_iris)\n",
    "print('Accuracy with Alpha 0.25:', accuracy_a025)\n",
    "\n",
    "# alpha = 0.75\n",
    "alpha = 0.75\n",
    "predictions_iris_a075 = (predictions_iris >= alpha).astype(int)\n",
    "\n",
    "accuracy_a075 = float(sum(predictions_iris_a075 == y_iris)) / len(y_iris)\n",
    "print('Accuracy with Alpha 0.75:', accuracy_a075)\n",
    "\n",
    "# alpha = 0.95\n",
    "alpha = 0.95\n",
    "predictions_iris_a095 = (predictions_iris >= alpha).astype(int)\n",
    "\n",
    "accuracy_a095 = float(sum(predictions_iris_a095 == y_iris)) / len(y_iris)\n",
    "print('Accuracy with Alpha 0.95:', accuracy_a095)\n",
    "\n",
    "print('The best accuracy comes from Alpha 0.5:', accuracy_a05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d8213a",
   "metadata": {
    "id": "ptw8WSDyZ6gv"
   },
   "source": [
    "The End. Please **save (make sure you saved!!!) and upload your rendered notebook in the Digital Campus**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1592dd",
   "metadata": {
    "id": "7_9bfMdTZ6gw"
   },
   "source": [
    "**Grading Schema**\n",
    "\n",
    "\n",
    "Question 1: 10\n",
    "\n",
    "Question 2: 10\n",
    "\n",
    "Question 3: 10\n",
    "\n",
    "Question 4: 40\n",
    "\n",
    "Question 5: 10\n",
    "\n",
    "Question 6: 20"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython3",
   "formats": "md:myst,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
